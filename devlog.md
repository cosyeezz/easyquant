# 开发日志 (2025-12-13)

## 概述

本日开发实现了 **全 WebSocket 驱动的实时监控架构**。为了彻底解决 HTTP 轮询带来的带宽浪费和延迟问题，我们将进程列表监控和事件详情推送全部升级为 WebSocket 实时流。现在，前端仅在初次加载时拉取历史数据，后续所有状态更新均为服务器主动推送，实现了零延迟的真实“事件驱动”体验。

同时，我们对系统架构进行了深度思考，明确了当前 **内存直通 (In-Memory Direct Pass-through)** 方案的高性能优势，并拒绝了引入 RocketMQ 等重型中间件的过度设计。

## 详细工作项

### 1. 全 WebSocket 事件流架构
- **核心重构**:
    - **进程列表实时化**: 后端实现了 `processes_broadcast_worker`，每 2 秒广播一次最新的进程状态到 `system.processes` 频道，彻底移除了前端对 `/api/v1/processes` 的轮询。
    - **事件推送实时化**: 修改了 `event_service.record_event` 核心函数。现在，每当事件写入数据库成功后，会自动广播到 `process.events.{process_name}` 频道。
- **前端适配**:
    - 升级 `useWebSocket` Hook，增加了对动态频道 `process.events.*` 的订阅支持。
    - 重构 `ProcessMonitor` 组件，实现了“历史+实时”混合模式：
        1. **选中进程时**: 仅调用一次 API 拉取历史事件。
        2. **即刻订阅**: 开启 WebSocket 监听，实时追加新产生的事件。
        3. **自动清理**: 切换进程时自动退订并清空队列，确保无内存泄漏。

### 2. 基础设施 (Infrastructure)
- **代码解耦与重构**:
    - 将 `server/common/websocket_manager.py` 净化为纯粹的通信组件，只保留连接管理与消息广播功能。
    - 将日志拦截 (`WebSocketLogHandler`) 和系统监控 (`system_status_worker`) 等具体业务逻辑迁移至 `server/api/v1/system.py`，实现了“基础设施”与“业务逻辑”的清晰分离。
- **数据库连接的智能 SSH 隧道**:
    - 解决了本地开发环境连接远程数据库的痛点。系统现在能智能识别运行环境（本地/远程），自动建立 SSH 隧道，无需维护多套配置。
    - 经基准测试 (`benchmark_comparison.py`)，确认内网直连（~1.7ms）比 SSH 隧道（~225ms）快 135 倍，确立了内网为生产环境标准。
- **WebSocket 性能调优**:
    - 确认了当前架构（FastAPI + asyncio + 内存队列）在单机高并发下的极致性能（亚毫秒级延迟），优于引入 MQ 的方案。

### 3. 前端交互体验优化 (UX Polish)
- **日志监控面板 (LogViewer)**:
    - **按需订阅策略**: 默认状态改为“折叠且不订阅”，彻底消除后台静默流量。
    - **用户控制权**: 增加了显式的“接收/暂停”切换开关。
    - **状态持久化**: 利用 `localStorage` 记录用户的订阅和折叠偏好，刷新页面后自动恢复现场。
    - **视觉优化**: 使用静态的状态指示点（绿/灰）替代旋转动画，降低视觉干扰。
- **稳定性修复**:
    - 修复了 `ProcessMonitor` 中因回调依赖导致的无限重渲染（White Screen）问题。
    - 修复了日志滚动在 DOM 渲染未完成时失效的问题（引入 `setTimeout(..., 0)` 宏任务延迟）。

### 4. ETL 画布编辑器 (原型)
- **技术选型**: 引入 `reactflow` 库，启动了从“线性列表编辑器”向“节点导向图编辑器 (Node-based Graph Editor)”的架构迁移。
- **架构设计**:
    - **Schema 传播**: 设计了基于 Context 的元数据流引擎，旨在实现上游节点（如 CSV Loader）的列信息自动传递给下游节点（如 Mapping），从而实现智能的配置体验。
    - **双模式切换**: 在 ETL 编辑器中增加了“列表模式”与“画布模式”的切换开关，保证新老功能的平滑过渡。
- **当前状态**: 原型开发中（Alpha），已实现基础画布渲染和节点拖拽，但遇到了状态管理导致的白屏问题，待下次迭代修复。

## 架构决策记录 (ADR)

**决策 1**: 暂不引入 RocketMQ/Kafka 等消息队列中间件。
**原因**:
1.  **运维成本**: 引入 MQ 需要额外部署 NameServer/Broker，破坏了项目“一键启动”的极简体验。
2.  **性能损耗**: 当前 Python 内存队列 + WebSocket 的路径最短，无网络序列化开销。引入 MQ 会增加网络跳数，反而增加延迟。
3.  **适用性**: 当前单体架构下，内存通信是最优解。

**决策 2**: 拒绝引入 Netty (Java) 作为网关。
**原因**:
1.  **技术栈割裂**: 引入 Java 生态会极大增加开发和运维复杂度。
2.  **性能冗余**: Python 的 `uvicorn` (基于 `uvloop`/`libuv`) 性能已足够强悍，足以支撑当前及未来中期的并发需求。

## 下一步计划
1.  **ETL Runner 实现**: 既然监控系统已完美就绪，下一步必须打通数据处理的核心——串联 Loader、Pipeline 和 DatabaseSaveHandler。